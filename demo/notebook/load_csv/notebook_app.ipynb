{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "resultHeight": 0
   },
   "source": "# imports\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import col, regexp_extract, lit\nfrom snowflake.snowpark import DataFrame\nimport streamlit as st\nimport altair as alt\nfrom datetime import datetime\nimport pandas as pd\n\n# get snowpark active session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "resultHeight": 0
   },
   "source": "def load_data(db_schema: str, wh_size: str, tbls_to_load:list[str]) -> list[str]:\n    # change the warehouse size\"\n    session.sql(f\"ALTER WAREHOUSE LOAD SET WAREHOUSE_SIZE='{wh_size}'\").collect()\n    query_ids: list[str] = []\n\n    # set the database schema\n    session.use_schema(db_schema)\n    \n    # truncate and load table in parallel using snowpark\n    for tbl in tbls_to_load:\n        # print(f\"loading {db_schema}.{tbl} from @{location}/{tbl.lower()}/ with {fmt_name} using {wh_size}\")\n        _ = session.sql(f\"TRUNCATE TABLE {tbl}\").collect()\n        job = session.sql(f\"\"\"\n            COPY INTO {db_schema}.{tbl}\n            FROM @{location}/{tbl.lower()}/\n            FILE_FORMAT = ( FORMAT_NAME = '{fmt_name}')\n            MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE\n            FORCE = TRUE\n            \"\"\").collect_nowait()\n        query_ids.append(job.query_id)\n    return query_ids",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "resultHeight": 0
   },
   "source": "def wait_till_end_and_scale_down(query_ids: list[str]) -> None:\n    # wait till copy async jobs ended on session\n    while True:\n        # check if there are copy into queries in running status for the current session\n        df = (\n            session.table_function(\n                \"MEETUP_GDDP.INFORMATION_SCHEMA.QUERY_HISTORY_BY_SESSION\",\n                result_limit=lit(10000)\n            )\n            .filter(\n                ~(col(\"EXECUTION_STATUS\").in_([\"SUCCESS\", \"FAILED_WITH_ERROR\", \"FAILED_WITH_INCIDENT\",\n                                              \"ABORTED\", \"DISCONNECTED\"]) )\n                & (col(\"QUERY_ID\").in_(query_ids))\n            )\n        )\n\n        if df.count() == 0:\n            break\n\n    # scale down the warehouse\n    session.sql(f\"ALTER WAREHOUSE LOAD SET WAREHOUSE_SIZE='X-SMALL'\").collect()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0f8fb1a0-53ee-45af-83c9-bdbe38d92823",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "def display_results(query_ids: list[str]) -> None:\n    from snowflake.snowpark.functions import col, regexp_extract\n    # get queries stats by table\n    regex_pattern = r'COPY INTO \\S+\\.\\S+\\.(\\S+)'\n    df = (\n            session.table_function(\n                \"MEETUP_GDDP.INFORMATION_SCHEMA.QUERY_HISTORY_BY_SESSION\",\n                result_limit=lit(10000)\n            )\n            .filter((col(\"QUERY_ID\").in_(query_ids)))\n            .select(\n                [\n                    \"SESSION_ID\",\n                    \"QUERY_ID\",\n                    \"QUERY_TEXT\",\n                    \"WAREHOUSE_SIZE\",\n                    \"START_TIME\",\n                    \"END_TIME\",\n                    (col(\"TOTAL_ELAPSED_TIME\") / 1000).alias(\"TOTAL_ELAPSED_TIME_SECONDS\"),\n                    (col(\"COMPILATION_TIME\") / 1000).alias(\"COMPILATION_TIME_SECONDS\"),\n                    (col(\"QUEUED_OVERLOAD_TIME\") / 1000).alias(\"QUEUED_OVERLOAD_TIME_SECONDS\"),\n                    \"ROWS_PRODUCED\",\n                    regexp_extract(col(\"QUERY_TEXT\"), regex_pattern, 1).alias(\"TABLE_NAME\")\n                ]\n            )\n        )\n\n\n    # Collect the DataFrame to Pandas\n    df_pandas = df.to_pandas()\n\n    # Display the total elapsed time as a Streamlit metric\n    total_duration_seconds = (pd.to_datetime(df_pandas['END_TIME']).max() - pd.to_datetime(df_pandas['START_TIME']).min()).total_seconds()\n    st.metric(label=\"Total Elapsed Time (seconds)\",\n              value=f\"{total_duration_seconds:.2f}\")\n\n    # Create the Altair bar chart\n    bar_chart = (\n        alt.Chart(df_pandas)\n        .mark_bar(color=\"#872D60\")\n        .encode(\n            x=alt.X(\"TABLE_NAME:N\", title=\"Table\", axis=alt.Axis(labelAngle=-90, labelLimit=0)),\n            y=alt.Y(\"TOTAL_ELAPSED_TIME_SECONDS:Q\", title=\"Duration in seconds)\"),\n            xOffset=alt.XOffset(\"WAREHOUSE_SIZE:N\")\n        )\n    )\n\n    # Display the Altair chart in Streamlit\n    st.altair_chart(bar_chart, use_container_width=True)\n\n    # Display the DataFrame in Streamlit\n    st.write(df_pandas)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e1f5458-0aa1-48c3-9ee5-e91c5981aab8",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "resultHeight": 595
   },
   "outputs": [],
   "source": "# file format and base location on external stage\n# wh_size = \"Medium\"\n# tbls_to_load = [\"CUSTOMER\", \"LINEITEM\", \"NATION\", \"ORDERS\", \"PART\", \"PARTSUPP\", \"SUPPLIER\", \"REGION\"]\n\nwh_size = \"X-Small\"\ntbls_to_load = [\"NATION\", \"REGION\"]\n\ndb_schema = \"MEETUP_GDDP.TPCH_SF100\"\nfmt_name = \"MEETUP_GDDP.UTILS.CSV_FMT1\"\nlocation = \"MEETUP_GDDP.UTILS.LANDING/tpch-sf100/csv\"\nview_results = True\n\nquery_ids = load_data(db_schema, wh_size, tbls_to_load)\nwait_till_end_and_scale_down(query_ids)\nif view_results:\n    display_results(query_ids)",
   "execution_count": null
  }
 ]
}