{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb29705-0888-4394-a1a3-9655fea9bfcf",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell1",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import col, regexp_extract, lit\n",
    "from snowflake.snowpark import DataFrame\n",
    "import streamlit as st\n",
    "import altair as alt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# get snowpark active session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de0c4e-23e3-4ba8-a99c-0bba3e44ad9d",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell5",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "def load_data(db_schema: str, wh_size: str, tbls_to_load:list[str]) -> list[str]:\n",
    "    # change the warehouse size\"\n",
    "    session.sql(f\"ALTER WAREHOUSE LOAD SET WAREHOUSE_SIZE='{wh_size}'\").collect()\n",
    "    query_ids: list[str] = []\n",
    "\n",
    "    # truncate and load table in parallel using snowpark\n",
    "    for tbl in tbls_to_load:\n",
    "        # print(f\"loading {db_schema}.{tbl} from @{location}/{tbl.lower()}/ with {fmt_name} using {wh_size}\")\n",
    "        _ = session.sql(f\"TRUNCATE TABLE {tbl}\").collect()\n",
    "        job = session.sql(f\"\"\"\n",
    "            COPY INTO {db_schema}.{tbl}\n",
    "            FROM @{location}/{tbl.lower()}/\n",
    "            FILE_FORMAT = ( FORMAT_NAME = '{fmt_name}')\n",
    "            MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE\n",
    "            FORCE = TRUE\n",
    "            \"\"\").collect_nowait()\n",
    "        query_ids.append(job.query_id)\n",
    "    return query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5397b-672f-4956-872e-326135a9766b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell2",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "def wait_till_end_and_scale_down(query_ids: list[str]) -> None:\n",
    "    # wait till copy async jobs ended on session\n",
    "    while True:\n",
    "        # check if there are copy into queries in running status for the current session\n",
    "        df = (\n",
    "            session.table_function(\n",
    "                \"MEETUP_GDDP.INFORMATION_SCHEMA.QUERY_HISTORY_BY_SESSION\",\n",
    "                result_limit=lit(10000)\n",
    "            )\n",
    "            .filter(\n",
    "                ~(col(\"EXECUTION_STATUS\").in_([\"SUCCESS\", \"FAILED_WITH_ERROR\", \"FAILED_WITH_INCIDENT\",\n",
    "                                              \"ABORTED\", \"DISCONNECTED\"]) )\n",
    "                & (col(\"QUERY_ID\").in_(query_ids))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if df.count() == 0:\n",
    "            break\n",
    "\n",
    "    # scale down the warehouse\n",
    "    session.sql(f\"ALTER WAREHOUSE LOAD SET WAREHOUSE_SIZE='X-SMALL'\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26475359-a2cd-4800-9b41-74eb0243a52e",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell4",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "def display_results(query_ids: list[str]) -> None:\n",
    "    from snowflake.snowpark.functions import col, regexp_extract\n",
    "    # get queries stats by table\n",
    "    regex_pattern = r'COPY INTO \\S+\\.\\S+\\.(\\S+)'\n",
    "    df = (\n",
    "            session.table_function(\n",
    "                \"MEETUP_GDDP.INFORMATION_SCHEMA.QUERY_HISTORY_BY_SESSION\",\n",
    "                result_limit=lit(10000)\n",
    "            )\n",
    "            .filter((col(\"QUERY_ID\").in_(query_ids)))\n",
    "            .select(\n",
    "                [\n",
    "                    \"SESSION_ID\",\n",
    "                    \"WAREHOUSE_SIZE\",\n",
    "                    \"START_TIME\",\n",
    "                    \"END_TIME\",\n",
    "                    (col(\"TOTAL_ELAPSED_TIME\") / 1000).alias(\"TOTAL_ELAPSED_TIME_SECONDS\"),\n",
    "                    (col(\"COMPILATION_TIME\") / 1000).alias(\"COMPILATION_TIME_SECONDS\"),\n",
    "                    (col(\"QUEUED_OVERLOAD_TIME\") / 1000).alias(\"QUEUED_OVERLOAD_TIME_SECONDS\"),\n",
    "                    \"ROWS_PRODUCED\",\n",
    "                    regexp_extract(col(\"QUERY_TEXT\"), regex_pattern, 1).alias(\"TABLE_NAME\")\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    # Collect the DataFrame to Pandas\n",
    "    df_pandas = df.to_pandas()\n",
    "\n",
    "    # Display the total elapsed time as a Streamlit metric\n",
    "    total_duration_seconds = (pd.to_datetime(df_pandas['END_TIME']).max() - pd.to_datetime(df_pandas['START_TIME']).min()).total_seconds()\n",
    "    st.metric(label=\"Total Elapsed Time (seconds)\",\n",
    "              value=f\"{total_duration_seconds:.2f}\")\n",
    "\n",
    "    # Create the Altair bar chart\n",
    "    bar_chart = (\n",
    "        alt.Chart(df_pandas)\n",
    "        .mark_bar(color=\"#872D60\")\n",
    "        .encode(\n",
    "            x=alt.X(\"TABLE_NAME:N\", title=\"Table\", axis=alt.Axis(labelAngle=-90, labelLimit=0)),\n",
    "            y=alt.Y(\"TOTAL_ELAPSED_TIME_SECONDS:Q\", title=\"Duration in seconds)\"),\n",
    "            xOffset=alt.XOffset(\"WAREHOUSE_SIZE:N\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Display the Altair chart in Streamlit\n",
    "    st.altair_chart(bar_chart, use_container_width=True)\n",
    "\n",
    "    # Display the DataFrame in Streamlit\n",
    "    st.write(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e612ed1-8e31-44ea-b267-0d40d7158a9e",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell7",
    "resultHeight": 805
   },
   "outputs": [],
   "source": [
    "# file format and base location on external stage\n",
    "wh_size = \"Medium\"\n",
    "# wh_size = \"X-Small\"\n",
    "db_schema = \"MEETUP_GDDP.TPCH_SF100\"\n",
    "tbls_to_load = [\"CUSTOMER\", \"LINEITEM\", \"NATION\", \"ORDERS\", \"PART\", \"PARTSUPP\", \"SUPPLIER\", \"REGION\"]\n",
    "#tbls_to_load = [\"NATION\", \"REGION\"]\n",
    "fmt_name = \"MEETUP_GDDP.UTILS.CSV_FMT1\"\n",
    "location = \"MEETUP_GDDP.UTILS.LANDING/tpch-sf100/csv\"\n",
    "view_results = True\n",
    "\n",
    "query_ids = load_data(db_schema, wh_size, tbls_to_load)\n",
    "wait_till_end_and_scale_down(query_ids)\n",
    "if view_results:\n",
    "    display_results(query_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
